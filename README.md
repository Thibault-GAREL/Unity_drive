# ğŸï¸ğŸ¤– Unity AI Learning to Drive on a Race Circuit with PPO Reinforcement Learning

![Unity](https://img.shields.io/badge/Unity-3D-black.svg)
![ML-Agents](https://img.shields.io/badge/ML--Agents-PPO-blue.svg)
![Reinforcement Learning](https://img.shields.io/badge/Reinforcement-Learning-green.svg)

![License](https://img.shields.io/badge/license-MIT-green.svg)  
![Contributions](https://img.shields.io/badge/contributions-welcome-orange.svg)  

![Gif of the agent](img_drive/Driving-maze-gif.gif)

## ğŸ“ Project Description
This project demonstrates **Reinforcement Learning** using **PPO (Proximal Policy Optimization)** in **Unity 3D** to train an AI agent to drive around a race circuit.  
The character learns car-like movement (forward, backward, turn left, turn right) using a **vision-based system** to navigate through checkpoints and complete laps through trial and error.

âš¡ï¸ Essentially, the AI learns racing skills autonomously by seeing the track and being rewarded for passing checkpoints and completing laps!

---

## âš™ï¸ Features

ğŸ§  **PPO Reinforcement Learning** â€” State-of-the-art algorithm for training intelligent agents.  

ğŸ‘ï¸ **Vision-Based System** â€” Agent uses visual perception to understand the track and navigate.  

ğŸš— **Car-like Controls** â€” Agent learns realistic driving mechanics (forward, backward, left turn, right turn).  

ğŸ **Checkpoint System** â€” Progress tracked through multiple checkpoints around the circuit.  

ğŸ® **Unity ML-Agents** â€” Leverages Unity's machine learning framework for 3D environments.  

ğŸ”„ **Lap Completion** â€” Agent learns to complete full circuit laps by reaching all checkpoints in order.  

ğŸ“Š **Performance Tracking** â€” Monitors checkpoint progress and lap completion.

---

## ğŸ“Š Example Outputs

### Trained Agent Performance

**Circuit Navigation:**
<p align="center">
  <img src="img_drive/Driving-maze-gif.gif" alt="Agent Driving on Circuit" width="700">
</p>

### Learning Challenges
During training, the agent learns from mistakes such as wall collisions and track deviations:

**Misaligned AI by crossing walls:**
<p align="center">
  <img src="img_drive/Driving-maze-gif-error.gif" alt="Misaligned AI by crossing walls" width="700">
</p>

### 3D Character Model
Custom Templar character model I created for this project (I wanted to create a model I can use for differente projects. I started with a chess pown, then a animal but I wanted a model without any need of movement so I made a Templar with different color to make team):

<p align="center">
  <img src="img_greedy/Templar_faceVF.jpg" alt="Templar Model - Front View" width="550">
  <img src="img_greedy/Templar_perspectiveVF.jpg" alt="Templar Model - Perspective View" width="550">
</p>

---

## ğŸ§  Training Visualization

The training process shows the agent learning to navigate the circuit:
- **Character agent** = AI-controlled entity learning to race  
- **Checkpoints** = Sequential targets to reach for lap completion  
- **Vision rays** = Visual perception system for track detection  
- **Circuit track** = Racing environment with curves and straightaways

As training progresses, the agent transitions from making frequent errors (wall collisions, missing checkpoints) to smooth, efficient driving. The vision-based system allows it to perceive track boundaries and adapt its racing line accordingly.

---

## âš™ï¸ How it Works

1. ğŸ¬ **Environment Reset** â†’ Agent spawns at the starting position on the circuit.  
2. ğŸ‘ï¸ **Visual Observation** â†’ The agent perceives the track through vision rays detecting track boundaries and checkpoints.  
3. ğŸ¤– **State Information** â†’ Agent processes its speed, rotation, and current checkpoint progress.  
4. ğŸ§  **Decision** â†’ PPO algorithm decides driving actions (forward/backward, turn left/right) based on visual input and current policy.  
5. ğŸš— **Action & Movement** â†’ Agent performs car-like movements to navigate the circuit.  
6. ğŸ **Checkpoint Rewards** â†’ Agent receives positive rewards for passing checkpoints in the correct order.  
7. ğŸ”„ **Learning** â†’ Neural network updates based on accumulated racing experiences.  
8. âœ… **Lap Completion** â†’ Agent learns to complete full laps by reaching all checkpoints sequentially.

The vision-based approach allows the agent to learn track features and adapt its driving strategy accordingly.

---

## ğŸ§° Components

| Component | Description |
|-----------|-------------|
| ğŸï¸ **Agent (Character)** | AI-controlled entity learning car-like driving on a circuit |
| ğŸ‘ï¸ **Vision System** | Ray-based perception for detecting track boundaries and environment |
| ğŸ **Checkpoint System** | Sequential waypoints marking progress around the circuit |
| ğŸ›£ï¸ **Race Circuit** | Track environment with curves, straights, and boundaries |
| ğŸ§  **PPO Algorithm** | Reinforcement learning algorithm optimizing racing behavior |
| ğŸ® **Car Controls** | Forward, backward, turn left, turn right actions |
| ğŸ“Š **Lap Counter** | Tracks checkpoint progress and completed laps |
| â±ï¸ **Time/Episode Limit** | Constraint for completing the circuit |

---

## ğŸš€ Getting Started

### Prerequisites
- Unity 2020.3 or later
- Unity ML-Agents Toolkit
- Python 3.7+ (for ML-Agents training)

### Installation
1. Clone this repository
2. Open the project in Unity
3. Install ML-Agents package via Package Manager
4. Configure training parameters in the YAML config file
5. Run training using `mlagents-learn` command

### Training Tips
- Adjust vision ray count and distance for optimal track perception
- Balance rewards between checkpoint progression and lap completion
- Fine-tune penalty for going off-track or wrong direction
- Experiment with different circuit layouts for varied training

---

## ğŸ“– Inspiration / Sources  

This project uses Unity's ML-Agents framework for reinforcement learning:
- [Unity ML-Agents Documentation](https://github.com/Unity-Technologies/ml-agents)
- [PPO Algorithm Paper](https://arxiv.org/abs/1707.06347)
- [Ray Perception Sensor Documentation](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Learning-Environment-Design-Agents.md#raycast-observations)

Project created by me ğŸ˜, Thibault GAREL - [Github](https://github.com/Thibault-GAREL)
